// backend-nlu-llm.js
import express from "express";
import mongoose from "mongoose";
import cors from "cors";
import dotenv from "dotenv";
import axios from "axios";
dotenv.config();

import { Annotation } from "./models/Annotation.js";

const app = express();
app.use(cors());
app.use(express.json());

const PORT = process.env.PORT || 5000;
const HF_API_KEY = process.env.HUGGING_FACE_API_KEY;

// --- Connexion MongoDB ---
mongoose
  .connect("mongodb://localhost:27017/maha")
  .then(() => console.log("‚úÖ MongoDB connect√©"))
  .catch((err) => console.error("‚ùå Erreur MongoDB :", err));

// --- Fonction NLU Hugging Face ---
async function queryNLU(sentence) {
  const model = "Rasa/rasa-nlu-bert-small-fr"; // mod√®le fran√ßais NLU
  try {
    const response = await axios.post(
      `https://api-inference.huggingface.co/models/${model}`,
      { inputs: sentence },
      {
        headers: { Authorization: `Bearer ${HF_API_KEY}` }
      }
    );
    return response.data; // { intent: ..., entities: [...] }
  } catch (err) {
    console.error("Erreur NLU HF :", err.response?.data || err.message);
    return null;
  }
}

// --- Fonction LLM Hugging Face ---
async function queryLLM(question, annotations = []) {
  const model = "openai/gpt-oss-20b:groq"; // ou un autre mod√®le LLM
  const context = annotations.join("\n\n");
  const systemPrompt = `
Tu es un guide virtuel √† l'ENSAM Mekn√®s.
R√©ponds uniquement √† partir des informations suivantes, sans inventer ni compl√©ter :

${context}

Question :
`;

  try {
    const response = await axios.post(
      "https://router.huggingface.co/v1/chat/completions",
      {
        model: model,
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: question }
        ]
      },
      {
        headers: {
          Authorization: `Bearer ${HF_API_KEY}`,
          "Content-Type": "application/json"
        }
      }
    );
    return response.data.choices[0]?.message?.content || "Je n'ai pas de r√©ponse.";
  } catch (err) {
    console.error("Erreur LLM HF :", err.response?.data || err.message);
    return "Erreur lors de la g√©n√©ration de la r√©ponse.";
  }
}

// --- Traitement du message ---
async function processChat(message, sceneId) {
  // --- NLU pour d√©tecter intention et entit√©s ---
  const nluResult = await queryNLU(message);

  // --- R√©cup√©ration annotations locales et globales ---
  const localAnnotation = sceneId
    ? await Annotation.findOne({ scene_id: sceneId }) // <-- correction ici
    : null;

  const globalAnnotations = await Annotation.find({ scene_id: "global" });
  const allAnnotations = [localAnnotation?.annotation, ...globalAnnotations.map(a => a.annotation)].filter(Boolean);

  if (allAnnotations.length === 0) {
    console.log(`‚ö†Ô∏è Aucune annotation trouv√©e pour sceneId="${sceneId}"`);
  }

  // --- Si NLU est compris ---
  if (nluResult && nluResult.intent) {
    return await queryLLM(message, allAnnotations);
  }

  // --- Sinon fallback LLM direct ---
  return await queryLLM(message, allAnnotations);
}

// --- API endpoint ---
app.post("/chat", async (req, res) => {
  const { message, sceneId } = req.body;
  if (!message) return res.status(400).json({ reply: "Message requis." });

  try {
    const reply = await processChat(message, sceneId);
    res.json({ reply });
  } catch (err) {
    console.error(err);
    res.status(500).json({ reply: "Erreur interne du serveur." });
  }
});

// --- Lancement serveur ---
app.listen(PORT, () => {
  console.log(`üöÄ Serveur pr√™t sur http://localhost:${PORT}`);
});
